{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMP\n",
    "\n",
    "In this lab you will be working with **OpenMP**.  Theis lab has four parts with increasing difficulty.\n",
    "\n",
    "You will be parallelizing a code that resizes a regular cube (model) through linear interpolation. The basic algorithm assumes that the output (data) space is larger by an integer scaling factor. The **forward** operator does  outer loops over the smaller input domain. To fill in the data for each portion of the model we use linear interpolation using the current and next value along each axis.\n",
    "\n",
    "The next cell compiles the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p build; cd build; rm -rf *;cmake -DCMAKE_CXX_COMPILER=g++ -DgenericIO_DIR=/opt/SEP/lib/cmake -DCMAKE_CXX_FLAGS=\"-O3\" -DCMAKE_INSTALL_PREFIX=/opt/SEP/local ../src; make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to create and plot a simple 2D model and plot one corner of it. You should not need to edit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import SepVector\n",
    "import LabOpenMP\n",
    "import Hypercube\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vecSmall = SepVector.getSepVector(Hypercube.hypercube(ns=[10001,10001]))\n",
    "vecBig = SepVector.getSepVector(Hypercube.hypercube(ns=[50000,50000]))\n",
    "\n",
    "mat = vecSmall.getNdArray()\n",
    "\n",
    "for i2 in range(mat.shape[0]):\n",
    "    for i1 in range(mat.shape[1]):\n",
    "        mat[i2][i1] = (500 - i2) * (500 - i2) + (500 - i1) * (500 - i1)\n",
    "\n",
    "x = mat[:100,:100]\n",
    "x = x - 320000\n",
    "plt.imshow(mat[:100,:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create our `rescale2D` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = LabOpenMP.rescale2D(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run and time the forward operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scale.forwardS(vecSmall, vecBig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot an equivalent portion of the regridded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "m = vecBig.getNdArray()[:500,:500]\n",
    "m = m - 320000\n",
    "\n",
    "plt.imshow(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs the \"dot product test\".  It checks that your adjoint operator is actually an adjoint to the forward operator. The printed values should be very close to each other (at least to the 5th decimal point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vecSmall.clone()\n",
    "y = vecBig.clone()\n",
    "x.rand()\n",
    "y.rand()\n",
    "yp = y.clone()\n",
    "xp = x.clone()\n",
    "scale.forwardS(x, yp)\n",
    "scale.adjointS(xp, y)\n",
    "print(x.dot(xp))\n",
    "print(y.dot(yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find a cell to edit the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/lib/rescale.cc\n",
    "#include <rescale.h>\n",
    "#include <cassert>\n",
    "#include <omp.h>\n",
    "\n",
    "using namespace gp257;\n",
    "\n",
    "void rescale2D::forwardS(const std::shared_ptr<float2DReg> mod,\n",
    "                         std::shared_ptr<float2DReg> dat) {\n",
    "  const std::shared_ptr<hypercube> hM = mod->getHyper(), hD = dat->getHyper();\n",
    "  assert((hM->getAxis(1).n - 1) * _rescale == hD->getAxis(1).n);\n",
    "  assert((hM->getAxis(2).n - 1) * _rescale == hD->getAxis(2).n);\n",
    "  float db = 1. / (float)_rescale;\n",
    "    \n",
    "  for (int i2 = 0; i2 < hM->getAxis(2).n - 1; i2++) {\n",
    "    for (int i1 = 0; i1 < hM->getAxis(1).n - 1; i1++) {\n",
    "      float b11 = (*mod->_mat)[i2][i1], b21 = (*mod->_mat)[i2][i1 + 1],\n",
    "            b12 = (*mod->_mat)[i2 + 1][i1], b22 = (*mod->_mat)[i2 + 1][i1 + 1];\n",
    "      float f2 = 0;\n",
    "      for (int ir2 = 0, id2 = i2 * _rescale; ir2 < _rescale;\n",
    "           ir2++, id2++, f2 += db) {\n",
    "        float f1 = 0;\n",
    "        for (int ir1 = 0, id1 = i1 * _rescale; ir1 < _rescale;\n",
    "             ir1++, id1++, f1 += db) {\n",
    "          (*dat->_mat)[id2][id1] = (1. - f1) * (1. - f2) * b11 +\n",
    "                                   (f1) * (1. - f2) * b21 +\n",
    "                                   (1. - f1) * (f2)*b12 + f1 * f2 * b22;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "        \n",
    "void rescale2D::forwardP(const std::shared_ptr<float2DReg> mod,\n",
    "                         std::shared_ptr<float2DReg> dat) {}\n",
    "\n",
    "void rescale2D::forwardP2(const std::shared_ptr<float2DReg> mod,\n",
    "                          std::shared_ptr<float2DReg> dat) {}\n",
    "        \n",
    "void rescale2D::adjointS(std::shared_ptr<float2DReg> mod,\n",
    "                         const std::shared_ptr<float2DReg> dat) {\n",
    "  const std::shared_ptr<hypercube> hM = mod->getHyper(), hD = dat->getHyper();\n",
    "  assert((hM->getAxis(1).n - 1) * _rescale == hD->getAxis(1).n);\n",
    "  assert((hM->getAxis(2).n - 1) * _rescale == hD->getAxis(2).n);\n",
    "  float db = 1. / (float)_rescale;\n",
    "\n",
    "  for (int i2 = 0; i2 < hM->getAxis(2).n; i2++) {\n",
    "    for (int i1 = 0; i1 < hM->getAxis(1).n; i1++) {\n",
    "      (*mod->_mat)[i2][i1] = 0;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  for (int i2 = 0; i2 < hM->getAxis(2).n - 1; i2++) {\n",
    "    for (int i1 = 0; i1 < hM->getAxis(1).n - 1; i1++) {\n",
    "      float f2 = 0;\n",
    "      for (int ir2 = 0, id2 = i2 * _rescale; ir2 < _rescale;\n",
    "           ir2++, id2++, f2 += db) {\n",
    "        float f1 = 0;\n",
    "        for (int ir1 = 0, id1 = i1 * _rescale; ir1 < _rescale;\n",
    "             ir1++, id1++, f1 += db) {\n",
    "          (*mod->_mat)[i2][i1] +=\n",
    "              (1. - f1) * (1. - f2) * (*dat->_mat)[id2][id1];\n",
    "          (*mod->_mat)[i2][i1 + 1] += (f1) * (1. - f2) * (*dat->_mat)[id2][id1];\n",
    "          (*mod->_mat)[i2 + 1][i1] += (1. - f1) * (f2) * (*dat->_mat)[id2][id1];\n",
    "          (*mod->_mat)[i2 + 1][i1 + 1] += (f1) * (f2) * (*dat->_mat)[id2][id1];\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void rescale2D::adjointP(std::shared_ptr<float2DReg> mod,\n",
    "              const std::shared_ptr<float2DReg> dat) {}\n",
    "\n",
    "void rescale2D::adjointP2(std::shared_ptr<float2DReg> mod,\n",
    "               const std::shared_ptr<float2DReg> dat) {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Compile the code and record the time to execute the serial code. Introduce \n",
    "OpenMP parallelization to the forward operator in `forwardP`. Begin by parallelizing over the `i1` axis.  Change the number of threads you are using from 1, 2, 4, 8, 16 by setting the environment variable using the cell magic `%env OMP_NUM_THREADS=<number of threads to use>`. Calculate the speedup in each case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize over the `i2` axis in `forwardP2`. Run the same scaling test.\n",
    "Did the choice of loop parallelization matter? Why or why not?  How close did you come to perfect scaling?  Speculate on why you achieved your scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelizing the adjoint operator is a more challenging problem. If you use the same approach as you did in the forward you are likely to get the incorrect result. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "One way to parallelize the adjoint operator is by introducing locks and/or single statements.  Choose one of these approaches to parallelize the adjoint in `adjointP`. Try to at least beat the serial code's performance time. Perform the same scaling test and comment on the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "One approach to avoid the race condition of the previous section is to use what is often referred to as red-black ordering, or more generally a colored ordering approach.  The basic concept of these methods is to break a parallel problem which has race conditions into multiple parallel problems each of which do not have a race condition. Paralelize the adjoint using a red-black ordering scheme in `adjointP2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint\n",
    "To understand how a red-black ordering system might work imagine we are attempting to do 1-D interpolation using the same algorithm.  We could note that our first cell updates model points 0 and 1, our second cell 1 and 2, and\n",
    "our third cell 2 and 3.  We could modify our loop to parallelize over all odd number cells without introducing a race condition, then parallelize over all of the even cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "By introducing thread parallelism you have changed our view of the roofline model.  When doing parallel reads you can achieve memory read speeds of 72 GB/s.  The machine you are working on has 24 cores. Remake the roofline model from the \"roofline\" lab for this machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the crossover point between bandwidth and flop limit changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the operational intensity of the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close did you come to peak performance? Speculate why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
