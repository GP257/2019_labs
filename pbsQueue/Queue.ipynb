{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "In this lab you will learn how to submit jobs to the CEES queuing system.  CEES uses a queuing sytem called [PBS](https://en.wikipedia.org/wiki/Portable_Batch_System).  The basic idea of a queuing system is to enable \"fair\" use of a shared resource.  When you want to run a job on the cluster you create a small file that describes your job. This file contains information like how many cores/nodes your job needs, how to run the job, where to store the stdout and stderr of the job, and whether to email you when the job is completed. You must also specify a queue to submit a job to. The CEES cluster has two overlapping types of queues. There are *named* queues which belong to specific users. In these queues you can run unlimited-length jobs.  All of the nodes that are currently not being used by the named queues are part of the *default* queue. Everyone that has access to the cluster can submit jobs to the default queue. The catch is that default queue jobs are limited to no more than two hours.\n",
    "\n",
    "When a job is submitted to PBS it recalculates every job's priority. Job priority is based on things like how many jobs you have run in the last three weeks and how big the named queue you are member of is.  When enough resources for the highest priority jobs become available, the job is started.\n",
    "\n",
    "# Getting started\n",
    "This lab has to be run on the cluster. At this state you should have an account on the CEES cluster. \n",
    "\n",
    "First you will have to add this lab to your git repository (check the Git assignment on how to do this). Then open a terminal on your local computer and log into the `cees-rcf` cluster by typing:\n",
    "\n",
    "```ssh username@cees-rcf.stanford.edu```\n",
    "\n",
    "It will prompt you for your Stanford password. \n",
    "\n",
    "Jupyter is already installed on the CEES servers, so all you have to do is add its path to your environment variables:\n",
    "\n",
    "* In your `~/.bashrc` file add this line:\n",
    "```\n",
    "export PATH=/opt/anaconda3/bin:$PATH\n",
    "```\n",
    "Remember to type `source ~/.bashrc` to load the changes.\n",
    "\n",
    "In order to run the notebook server remotely, but use a local browser to interact with the notebook, follow these extra steps:\n",
    "\n",
    "* Create a Jupyter config file:\n",
    "\n",
    "```jupyter notebook --generate-config```\n",
    "\n",
    "* Add the following lines to the config file:\n",
    "```python\n",
    "c = get_config()\n",
    "c.NotebookApp.open_browser = False\n",
    "c.NotebookApp.port = <REMOTE-PORT-NUMBER>\n",
    "c.NotebookApp.password = ''\n",
    "```\n",
    "Replace REMOTE-PORT-NUMBER with the port number of your choice.\n",
    "\n",
    "\n",
    "* Run the jupyter notebook using screen\n",
    "```bash\n",
    "screen -R jupyter-notebook\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "* Port forwarding:\n",
    "On your local machine, use ssh to map a local port to the remote port:\n",
    "\n",
    "```ssh -N -f -L <LOCAL-PORT-NUMBER>:localhost:<REMOTE-PORT-NUMBER> username@cees-rcf.stanford.edu```\n",
    "\n",
    "* Access http://localhost:LOCAL-PORT-NUMBER from your local browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a parallel job\n",
    "\n",
    "In this lab you will building a Python class that submits and monitors jobs through PBS.  We will begin by creating a generic job class.  For any job we will define four classes:\n",
    " - `preJob`: How to prepare a job to be run\n",
    " - `checkJobFinishedCorrectly`: Check if a job finished correctly\n",
    " - `returnJobCommand`: Return a string containing how to run the job\n",
    " - `postJob`: Stuff to do after the job is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myJob:\n",
    "    \"\"\"A generalized class for describing a parallel job\"\"\"\n",
    "    def __init__(self,tag):\n",
    "        \"\"\"Initialize a job\"\"\"\n",
    "        self.tag = tag\n",
    "    \n",
    "    def preJob(self):\n",
    "        \"\"\"How to prepare a job to be run\"\"\"\n",
    "        #By default we don't need to do anything\n",
    "    \n",
    "    def checkJobFinishedCorrectly(self):\n",
    "        \"\"\"A routine to see if a job finished correctly\"\"\"\n",
    "        return True\n",
    "        \n",
    "    def returnJobCommand(self):\n",
    "        \"\"\"Return a string containing how to run the job, stdout, stderr\"\"\"\n",
    "        \n",
    "    def postJob(self):\n",
    "        \"\"\"Stuff I need to do after a job has run\"\"\"\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a generic class for running a parallel job. We will initialize the class by creating a dictionary of jobs it needs to run.  We define the function `runJobs`  which will run all the parallel tasks, making sure than no more than `maxJobsRunning` are running simultaneously and delaying the start time between jobs by `sleepTime`.  It will also define how to check that jobs are finnished.  It will also define how to check that jobs are finished. \n",
    "\n",
    "The class definition:\n",
    "  - `startJob`:  How to start a given job\n",
    "  - `checkJobsRunning`: A function to check to see if a job is running\n",
    "  - `allJobsFinished`: What to do after all jobs are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class runParallelJob:\n",
    "    \"\"\"A generalized base class for running a series of jobs in parallel\"\"\"\n",
    "    def __init__(self, tags):\n",
    "        \"\"\"Initialization of the base class\n",
    "        tags is a dictionary where tag->job\"\"\"\n",
    "        self.tags = tags\n",
    "        self.jobsRunning = {}\n",
    "        self.failed = {}\n",
    "\n",
    "    def startJob(self, key, str):\n",
    "        \"\"\"How to start a job\"\"\"\n",
    "        #Needs to be overwritten\n",
    "        raise Exception(\"Need to override how to startJob\")\n",
    "  \n",
    "    def checkJobsRunning(self):\n",
    "        \"\"\"Check to see what jobs are running \"\"\" \n",
    "        #Force this to be overwritten\n",
    "        raise Exception(\"checkJobsRunning must be overwritten\")\n",
    "    \n",
    "    def checkJobsFinished(self, finished): \n",
    "        \"\"\"Check to see if the jobs finished correctly\"\"\"\n",
    "        for job in finished:\n",
    "            if not self.jobsRunning[job].checkJobFinishedCorrectly():\n",
    "                #check to see if the job failed before\n",
    "                if self.failed.count(job) == 0:\n",
    "                    self.failed[job] = 0;\n",
    "                #update the count of failed job\n",
    "                self.failed[job] += 1\n",
    "                #if the job has failed more than twice give up on it\n",
    "                if self.failed[job] > 2:\n",
    "                    print (\"Giving up on %s\"%job)\n",
    "                #try to run to job again\n",
    "                else:\n",
    "                    self.tags[job] = self.jobsRunning[job]\n",
    "            del self.jobsRunning[job]\n",
    "      \n",
    "    def allJobsFinished(self):\n",
    "        \"\"\"What to do when all the jobs are finished\"\"\"\n",
    "\n",
    "    def runJobs(self,sleepTime,maxJobsRunning):\n",
    "        \"\"\"Run a series of parallel jobs\"\"\"\n",
    "        while len(self.tags) > 0 or len(self.jobsRunning) >0:\n",
    "            jobsFinished = self.checkJobsRunning()\n",
    "            self.checkJobsFinished(jobsFinished)\n",
    "            if len(self.jobsRunning) < maxJobsRunning and len(self.tags) > 0:\n",
    "                key, job = self.tags.popitem()\n",
    "                print(\"Starting job \", key)\n",
    "                self.jobsRunning[key] = job\n",
    "                c, o, e = job.returnJobCommand()\n",
    "                self.startJob(key, c, o, e)\n",
    "            time.sleep(sleepTime)\n",
    "        self.allJobsFinished()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, below you will find an example of the parallel job script. In this case all the jobs are run on a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "class singleNodeParallel(runParallelJob):\n",
    "    \"\"\"Run many jobs simultaneously on a single node\"\"\"\n",
    "    def __init__(self, tags):\n",
    "        runParallelJob.__init__(self, tags)\n",
    "        self.processPoll = {}\n",
    "\n",
    "    def startJob(self, key, command, stdo, stde):\n",
    "        if stde:\n",
    "            if stdo:\n",
    "                efile = open(stde, \"w\")\n",
    "                ofile = open(stdo, \"w\")\n",
    "                self.processPoll[key] = subprocess.Popen(command, stderr=efile, stdout=ofile, shell=True)\n",
    "            else:\n",
    "                efile = open(stde, \"w\");\n",
    "                self.processPoll[key] = subprocess.Popen(command, stderr=efile, shell=True)\n",
    "        else:\n",
    "            if stdo:\n",
    "                ofile=open(stdo,\"w\");\n",
    "                self.processPoll[key] = subprocess.Popen(command, stdout=ofile, shell=True)\n",
    "            else:\n",
    "                self.processPoll[key] = subprocess.Popen(command, shell=True)\n",
    "\n",
    "    def checkJobsRunning(self):\n",
    "        finished = []\n",
    "        for job, p in list(self.processPoll.items()):\n",
    "            if p.poll() is not None:\n",
    "                finished.append(job)\n",
    "                del self.processPoll[job] \n",
    "        return finished    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a simple program that estimates $\\pi$ to demonstrate our `singleNodeParallel` class. This simple script estimates $\\pi$ by randomly choosing points in a 2-D interval and then calculating what fraction are inside a circle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile piEstimate.py\n",
    "import random\n",
    "import sys\n",
    "\n",
    "def calcPi(ntrys):\n",
    "    inside = 0\n",
    "    for i in range(ntrys):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        radius = x * x + y * y\n",
    "        if radius <= 1:\n",
    "            inside += 1\n",
    "    return 4. * inside / ntrys\n",
    "\n",
    "print(calcPi(int(sys.argv[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a class inherited from our `myJob` class for running `piEstimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "class piCalc(myJob):\n",
    "    def __init__(self, nestimate, tag):\n",
    "        myJob.__init__(self, tag);\n",
    "        self.nestimate = nestimate\n",
    "        self.tag = tag\n",
    "    \n",
    "    def returnJobCommand(self):\n",
    "        stdo = \"/tmp/%s.%d\"%(os.environ[\"USER\"], self.tag)\n",
    "        command = \"python piEstimate.py %d \"%(self.nestimate)\n",
    "        return command, stdo, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can inherit from our `singleNodeParallel` class to run the job in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class piParallel(singleNodeParallel):\n",
    "    def __init__(self, nestimate, njobs):\n",
    "        jobs = {}\n",
    "        self.nc = njobs;\n",
    "        for i in range(njobs):\n",
    "            jobs[i] = piCalc(nestimate, i)\n",
    "        singleNodeParallel.__init__(self, jobs)\n",
    "    \n",
    "    def allJobsFinished(self):\n",
    "        tot = []\n",
    "        for i in range(self.nc):\n",
    "            f = open(\"/tmp/%s.%d\"%(os.environ[\"USER\"], i))\n",
    "            lines = f.readlines()\n",
    "            tot.append(float(lines[0].strip()))\n",
    "        sum = 0.\n",
    "        for v in tot:\n",
    "            sum += v\n",
    "        print(sum / self.nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run our `piParallel` class. In this case we are going to run at maximum two jobs so we don't overwhelm the head node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = piParallel(10000000, 2)\n",
    "job.runJobs(.5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the grid engine\n",
    "\n",
    "As mentioned earlier you submit jobs to the grid engine by writing small scripts. Lets begin by creating a class that can write one of these shell scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jobCreator:\n",
    "    \"\"\"Class for creating job scripts for PBS\"\"\"\n",
    "    def __init__(self, **kw):\n",
    "        \"\"\"Initialize the job creator class and default most parameters\"\"\"\n",
    "        self.initializeDefaultParams()\n",
    "        self.overrideDefaultParams(kw)\n",
    "      \n",
    "    def createJobScript(self, fileout, **kw):\n",
    "        \"\"\"We can override any parameters then write out file\"\"\"\n",
    "        self.overrideDefaultParams(kw)\n",
    "        self.writeFile(fileout)\n",
    "    \n",
    "    def initializeDefaultParams(self):\n",
    "        \"\"\"Setup some defaults\"\"\"\n",
    "        self.params = {}\n",
    "        self.params[\"name\"] = os.environ[\"USER\"] + \"_\" + \"awesomeness\" #name of the job in the queue\n",
    "        self.params[\"nodes\"] = 1\n",
    "        self.params[\"cores\"] = 1\n",
    "        self.params[\"queue\"] = \"default\"\n",
    "        self.params[\"stdout\"] = \"/data/cees/%s/stdout\"%os.environ[\"USER\"]\n",
    "        self.params[\"stderr\"] = \"/data/cees/%s/stderr\"%os.environ[\"USER\"]\n",
    "\n",
    "    def overrideDefaultParams(self, lst):\n",
    "        \"\"\"Override the defaults\"\"\"\n",
    "        for k,v in lst.items():\n",
    "            self.params[k] = v\n",
    "          \n",
    "    def writeFile(self, fileout):\n",
    "        \"\"\"Write job file\"\"\"\n",
    "        if \"commands\" not in self.params:\n",
    "            raise Exception(\"Must specify commands in either initialization or createJobScript\")\n",
    "        try: \n",
    "            f = open(fileout, \"w\")\n",
    "        except:\n",
    "            raise Exception(\"Could not open \"%fileout)\n",
    "        f.write(\"#!/bin/bash\\n\")\n",
    "        f.write(\"#PBS -N %s\\n\"%self.params[\"name\"])\n",
    "        f.write(\"#PBS -q %s\\n\"%self.params[\"queue\"])\n",
    "        f.write(\"#PBS -l nodes=%d:ppn=%d\\n\"%(int(self.params[\"nodes\"]), int(self.params[\"cores\"])))\n",
    "        f.write(\"#PBS -e %s\\n\"%self.params[\"stderr\"])\n",
    "        f.write(\"#PBS -o %s\\n\"%self.params[\"stdout\"])\n",
    "        if \"mail\" in self.params:\n",
    "            f.write(\"#PBS -M\"%self.params[\"mail\"])\n",
    "        if type(self.params[\"commands\"]) is list:\n",
    "            for c in self.params[\"commands\"]:\n",
    "                f.write(\"%s\\n\"%c)\n",
    "        elif type(self.params[\"commands\"]) is str:\n",
    "            f.write(\"cd $PBS_O_WORKDIR\\n\")\n",
    "            f.write(\"%s\\n\"%self.params[\"commands\"])\n",
    "        else:\n",
    "            raise Exception(\"Commands must be a list or string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Your job is to inherit from the `runParallelJob` class a `pbsParallelJob` class. You need to write the class so that it can submit and monitor jobs. When you are ready to test your class have each `piEstimate` function test 10000000 numbers. Run a total of 50 jobs, running up to 10 jobs simultaneously.  To help you with your task you will find two functions below that submit a job to the PBS engine and get the status of jobs currently being run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def submitJob(script):\n",
    "    \"\"\"Submit job return job number\"\"\"\n",
    "    out = subprocess.check_output([\"qsub\", script])\n",
    "    return out.decode(\"utf-8\").split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def returnJobStatus(queue):\n",
    "    \"\"\"Return dictionaryjob id-> [error, running, funished, queued]\"\"\"\n",
    "    lines=subprocess.check_output([\"qstat\"]).decode(\"utf-8\").split(\"\\n\")\n",
    "    status={}\n",
    "    lines.pop(0)\n",
    "    lines.pop(0)\n",
    "    for line in lines:\n",
    "        parts=line.split()\n",
    "        if len(parts)> 1:\n",
    "            ids=parts[0].split(\".\")\n",
    "            if parts[5] == queue:\n",
    "                status[ids[0]]=\"finished\"\n",
    "                if len(ids) > 1:\n",
    "                    if parts[4] == \"Q\":\n",
    "                        status[ids[0]]=\"queued\"\n",
    "                    elif parts[4]== \"R\":\n",
    "                        status[ids[0]]=\"running\"\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra credit (required for SEPers)\n",
    "\n",
    "What you have developed at this stage will run jobs on a single queue.  If you really want to maximize the amount of work you can get done you will submit jobs to multiple queues (the default plus a named queue).   You want to follow this strategy because the default queue represents more potential resources but your priority is higher on your named queue. Make a new class, `ultimatePbsParallelJob` that runs on multiple queues. Have your script submit enough jobs on your named queue until a job is waiting. Once a job is waiting on the named queue have it submit jobs on the default queue until a job is waiting. Continue to monitor the queues, trying to keep three jobs waiting in both queues until all jobs have completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
